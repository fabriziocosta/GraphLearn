{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import eden\n",
    "import matplotlib.pyplot as plt\n",
    "from eden.util import configure_logging\n",
    "import logging\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import tee, chain, islice\n",
    "import numpy as np\n",
    "import random\n",
    "from time import time\n",
    "import datetime\n",
    "from graphlearn.graphlearn import GraphLearnSampler\n",
    "from eden.util import fit,estimate\n",
    "from eden.graph import Vectorizer\n",
    "# get data\n",
    "from eden.converter.graph.gspan import gspan_to_eden\n",
    "from eden.converter.molecule.obabel import mol_file_to_iterable\n",
    "from eden.converter.molecule.obabel import obabel_to_eden\n",
    "from itertools import islice\n",
    "\n",
    "def get_graphs(dataset_fname, size=None):\n",
    "    iterable = mol_file_to_iterable(dataset_fname, file_format='smi')\n",
    "    graphs = obabel_to_eden(iterable, file_format='smi')\n",
    "    return islice(graphs,size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#rename to pre_processor and expose all relevant parameters for optimization\n",
    "\n",
    "def generate_sample(graphs,\n",
    "                    random_state=42,\n",
    "                    complexity=5,\n",
    "                    nu=0.25,\n",
    "                    radius_list=[0,1],\n",
    "                    thickness_list=[2,3],\n",
    "                    n_steps=5,\n",
    "                    n_samples=4,\n",
    "                    burnin=1,\n",
    "                    improving_threshold=0.25,\n",
    "                    max_core_size_diff=3):\n",
    "    graphs, graphs_ = tee(graphs)\n",
    "    sampler=GraphLearnSampler(radius_list=radius_list,thickness_list=thickness_list,\n",
    "                              min_cip_count=2, min_interface_count=2,\n",
    "                              vectorizer=Vectorizer(complexity), random_state=random_state)\n",
    "    \n",
    "    sampler.fit(graphs, nu=nu, n_jobs=-1)\n",
    "\n",
    "    logger.info('graph grammar stats:')\n",
    "    dataset_size, interface_counts, core_counts, cip_counts = sampler.grammar().size()\n",
    "    logger.info('#instances:%d   #interfaces: %d   #cores: %d   #core-interface-pairs: %d' % (dataset_size, interface_counts, core_counts, cip_counts))\n",
    "    graphs = sampler.sample(graphs_,\n",
    "                            n_steps=n_steps, \n",
    "                            n_samples=n_samples,\n",
    "                            target_orig_cip=True,\n",
    "                            probabilistic_core_choice=False,\n",
    "                            score_core_choice= False,\n",
    "                            max_core_size_diff=max_core_size_diff,\n",
    "                            burnin=burnin,\n",
    "                            omit_seed=True,\n",
    "                            max_cycle_size=6,\n",
    "                            improving_threshold=improving_threshold,\n",
    "                            accept_static_penalty=0,\n",
    "                            n_jobs=-1,\n",
    "                            select_cip_max_tries=200,\n",
    "                            keep_duplicates=True,\n",
    "                            generator_mode=True)\n",
    "    return graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def constructive_model(pos_fname, neg_fname, size=None, model_fname=None, n_iter=40, train_test_split=0.7, random_state=42):\n",
    "    def pre_processor( graphs, **args):\n",
    "        graphs = generate_sample(graphs, **args)\n",
    "        return graphs\n",
    "    \n",
    "    from eden.graph import Vectorizer\n",
    "    vectorizer = Vectorizer()\n",
    "\n",
    "    from sklearn.linear_model import SGDClassifier\n",
    "    estimator = SGDClassifier(average=True, class_weight='auto', shuffle=True)\n",
    "\n",
    "    #create iterable from files\n",
    "    iterable_pos= get_graphs(pos_fname, size=size)\n",
    "    iterable_neg= get_graphs(neg_fname, size=size)\n",
    "\n",
    "\n",
    "    from itertools import tee\n",
    "    iterable_pos, iterable_pos_ = tee(iterable_pos)\n",
    "    iterable_neg, iterable_neg_ = tee(iterable_neg)\n",
    "    \n",
    "    import time\n",
    "    start = time.time()\n",
    "    logger.info('# positives: %d  # negatives: %d (%.1f sec %s)'%(sum(1 for x in iterable_pos_), sum(1 for x in iterable_neg_), time.time() - start, str(datetime.timedelta(seconds=(time.time() - start)))))\n",
    "    \n",
    "    #split train/test\n",
    "    from eden.util import random_bipartition_iter\n",
    "    iterable_pos_train, iterable_pos_test = random_bipartition_iter(iterable_pos, relative_size=train_test_split)\n",
    "    iterable_neg_train, iterable_neg_test = random_bipartition_iter(iterable_neg, relative_size=train_test_split)\n",
    "\n",
    "\n",
    "\n",
    "    #make predictive model\n",
    "    #NOTE: since parallelization cannot happen in a nested way, and since the graph learn already parallelize, we avoid \n",
    "    from eden.model import ActiveLearningBinaryClassificationModel\n",
    "    model = ActiveLearningBinaryClassificationModel(pre_processor,\n",
    "                                                    estimator=estimator,\n",
    "                                                    vectorizer=vectorizer,\n",
    "                                                    pre_processor_n_jobs=1,\n",
    "                                                    random_state=random_state)\n",
    "\n",
    "    #optimize hyperparameters and fit model\n",
    "    from numpy.random import randint\n",
    "    from numpy.random import uniform\n",
    "\n",
    "    pre_processor_parameters={'complexity':[5],\n",
    "                              'nu':[0.1,0.25,0.33,0.5],\n",
    "                              'radius_list':[[0,1],[0,1,2]],\n",
    "                              'thickness_list':[[1],[1,2],[2],[2,3]],\n",
    "                              'n_steps':[5,10,40],\n",
    "                              'n_samples':[4],\n",
    "                              'burnin':[1,5],\n",
    "                              'improving_threshold':[0.25,0.5,0.75],\n",
    "                              'max_core_size_diff':[0,1,3],\n",
    "                              'random_state':[random_state]} \n",
    "\n",
    "    vectorizer_parameters={'complexity':[4]}\n",
    "\n",
    "    estimator_parameters={'n_iter':randint(5, 100, size=n_iter),\n",
    "                          'penalty':['l1','l2','elasticnet'],\n",
    "                          'l1_ratio':uniform(0.1,0.9, size=n_iter), \n",
    "                          'loss':['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n",
    "                          'power_t':uniform(0.1, size=n_iter),\n",
    "                          'alpha': [10**x for x in range(-8,-2)],\n",
    "                          'eta0': [10**x for x in range(-4,-1)],\n",
    "                          'learning_rate': [\"invscaling\", \"constant\", \"optimal\"]}\n",
    "\n",
    "    model.optimize(iterable_pos_train, iterable_neg_train, \n",
    "                   model_name=model_fname,\n",
    "                   n_iter=n_iter,\n",
    "                   pre_processor_parameters=pre_processor_parameters, \n",
    "                   vectorizer_parameters=vectorizer_parameters, \n",
    "                   estimator_parameters=estimator_parameters)\n",
    "  \n",
    "    #estimate predictive performance\n",
    "    model.estimate( iterable_pos_test, iterable_neg_test, cv=10 )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Experimental pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "configure_logging(logger,verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# positives: 100  # negatives: 100 (0.2 sec 0:00:00.165185)\n",
      "graph grammar stats:\n",
      "#instances:70   #interfaces: 47   #cores: 75   #core-interface-pairs: 174\n"
     ]
    }
   ],
   "source": [
    "pos_fname='bursi_pos_orig.smi'\n",
    "neg_fname='bursi_neg_orig.smi'\n",
    "model = constructive_model(pos_fname, neg_fname, size=100, model_fname='bursi', n_iter=5, train_test_split=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicit tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Params:\n",
      "    burnin: 1\n",
      "complexity: 5\n",
      "improving_threshold: 0.25\n",
      "max_core_size_diff: 3\n",
      " n_samples: 4\n",
      "   n_steps: 5\n",
      "        nu: 0.25\n",
      "radius_list: [0, 1]\n",
      "random_state: 42\n",
      "thickness_list: [2, 3]\n",
      "--------------------------------------------------------------------------------\n",
      "Grammar induction:\n",
      "Positives:\n",
      "graph grammar stats:\n",
      "#instances:70   #interfaces: 32   #cores: 35   #core-interface-pairs: 84\n",
      "Time elapsed: 0:00:09.151178\n",
      "Negatives:\n",
      "graph grammar stats:\n",
      "#instances:70   #interfaces: 53   #cores: 39   #core-interface-pairs: 135\n",
      "Time elapsed: 0:00:08.018887\n",
      "--------------------------------------------------------------------------------\n",
      "Fitting:\n",
      "Time elapsed: 0:01:57.051407\n",
      "--------------------------------------------------------------------------------\n",
      "Testing:\n",
      "Test set\n",
      "Instances: 60 ; Features: 1048577 with an avg of 463 features per instance\n",
      "--------------------------------------------------------------------------------\n",
      "Test Estimate\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.86      0.60      0.71        30\n",
      "          1       0.69      0.90      0.78        30\n",
      "\n",
      "avg / total       0.77      0.75      0.74        60\n",
      "\n",
      "APR: 0.734\n",
      "ROC: 0.816\n",
      "Cross-validated estimate\n",
      "            accuracy: 0.633 +- 0.085\n",
      "           precision: 0.603 +- 0.065\n",
      "              recall: 0.767 +- 0.170\n",
      "                  f1: 0.670 +- 0.098\n",
      "   average_precision: 0.754 +- 0.082\n",
      "             roc_auc: 0.733 +- 0.084\n",
      "Time elapsed: 0:00:03.511154\n",
      "Global time elapsed: 0:02:17.905071\n",
      "CPU times: user 11.7 s, sys: 4.93 s, total: 16.6 s\n",
      "Wall time: 2min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#explicit experiment\n",
    "start_global = time()\n",
    "\n",
    "#train a model on data, then test it on original data (different from the mols that generated the data) and compare \n",
    "from eden.graph import Vectorizer\n",
    "vectorizer=Vectorizer(5)\n",
    "\n",
    "#setup\n",
    "size=100\n",
    "pos_fname='bursi_pos_orig.smi'\n",
    "neg_fname='bursi_neg_orig.smi'\n",
    "iterable_pos= get_graphs(pos_fname, size=size)\n",
    "iterable_neg= get_graphs(neg_fname, size=size)\n",
    "random_state=42\n",
    "train_test_split=.7\n",
    "\n",
    "#split train/test\n",
    "from eden.util import random_bipartition_iter\n",
    "iterable_pos_train, iterable_pos_test = random_bipartition_iter(iterable_pos, relative_size=train_test_split)\n",
    "iterable_neg_train, iterable_neg_test = random_bipartition_iter(iterable_neg, relative_size=train_test_split)\n",
    "\n",
    "args = {'random_state':42,\n",
    "        'complexity':5,\n",
    "        'nu':0.25,\n",
    "        'radius_list':[0,1],\n",
    "        'thickness_list':[2,3],\n",
    "        'n_steps':5,\n",
    "        'n_samples':4,\n",
    "        'burnin':1,\n",
    "        'improving_threshold':0.25,\n",
    "        'max_core_size_diff':3}\n",
    "            \n",
    "logger.info('-'*80)\n",
    "logger.info('Params:')\n",
    "from eden.util import serialize_dict\n",
    "logger.info(serialize_dict(args))\n",
    "\n",
    "#train\n",
    "start = time()\n",
    "logger.info('-'*80)\n",
    "logger.info('Grammar induction:')\n",
    "logger.info('Positives:')\n",
    "sampled_pos = generate_sample(iterable_pos_train, **args)\n",
    "logger.info('Time elapsed: %s'%(datetime.timedelta(seconds=(time() - start))))\n",
    "\n",
    "start = time()\n",
    "logger.info('Negatives:')\n",
    "sampled_neg = generate_sample(iterable_neg_train, **args)\n",
    "print('Time elapsed: %s'%(datetime.timedelta(seconds=(time() - start))))\n",
    "\n",
    "start = time()\n",
    "logger.info('-'*80)\n",
    "logger.info('Fitting:')\n",
    "from eden.util import fit\n",
    "estimator = fit(sampled_pos, \n",
    "                sampled_neg, \n",
    "                vectorizer, \n",
    "                fit_flag=False, \n",
    "                n_jobs=-1, \n",
    "                cv=10, \n",
    "                n_iter_search=1, \n",
    "                random_state=1, \n",
    "                block_size=100)\n",
    "logger.info('Time elapsed: %s'%(datetime.timedelta(seconds=(time() - start))))\n",
    "\n",
    "\n",
    "#test\n",
    "start = time()\n",
    "logger.info('-'*80)\n",
    "logger.info('Testing:')\n",
    "from eden.util import estimate\n",
    "apr, roc = estimate(iterable_pos_test,\n",
    "                    iterable_neg_test,  \n",
    "                    estimator, \n",
    "                    vectorizer, \n",
    "                    block_size=100, \n",
    "                    n_jobs=-1)\n",
    "logger.info('Time elapsed: %s'%(datetime.timedelta(seconds=(time() - start))))\n",
    "\n",
    "logger.info('Global time elapsed: %s'%(datetime.timedelta(seconds=(time() - start_global))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
