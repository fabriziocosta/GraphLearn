{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from eden.util import configure_logging\n",
    "import logging\n",
    "configure_logging(logging.getLogger(),verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time\n",
    "'''\n",
    "first we test the core/interface extractor, since it is most essential\n",
    "'''\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "%matplotlib inline\n",
    "# output workaround, sometimes necessary\n",
    "#import matplotlib\n",
    "#matplotlib.use('Agg')\n",
    "from eden.converter.graph.gspan import gspan_to_eden\n",
    "from graphlearn.graphlearn import GraphLearnSampler\n",
    "from eden.graph import Vectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from eden.util import fit, random_bipartition_iter , fit_estimator\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import random\n",
    "\n",
    "\n",
    "# a vectorizer\n",
    "vectorizer = Vectorizer( complexity=3 )\n",
    "\n",
    "# select 1st element in an iterator\n",
    "def unpack(graphs):\n",
    "    for graphlist in graphs:\n",
    "        yield graphlist[0]\n",
    "\n",
    "# positive set contains 2401 elements, of which we use 30% to test of we cen improve them ,\n",
    "# the rest is used for the oracle\n",
    "lenpo=int(2401*.3)\n",
    "\n",
    "\n",
    "sampler_graph,oracle = random_bipartition_iter(gspan_to_eden('bursi.pos.gspan'),.3)\n",
    "\n",
    "# we create an oracle\n",
    "#estimator=make_estimator(oracle,gspan_to_eden('bursi.neg.gspan'))\n",
    "X=vectorizer.transform(oracle)\n",
    "X_=X.multiply(-1)\n",
    "estimator= fit_estimator(SGDClassifier(),X,X_)\n",
    "print 'estimator ok'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "\n",
    "# ok we create an iterator over the graphs we want to work with... \n",
    "graphs_pos= sampler_graph\n",
    "\n",
    "\n",
    "# we want to use an increasing part of the test set.. \n",
    "percentages=[0.1, 0.25, 0.5, 0.75, 0.9]\n",
    "percentages=[0.1]\n",
    "\n",
    "\n",
    "sampler = GraphLearnSampler(radius_list=[1,2],thickness_list=[1,2])\n",
    "orires=[]\n",
    "samplres=[]\n",
    "for perc in percentages:\n",
    "    \n",
    "    originals=[]\n",
    "    samples=[]\n",
    "    for e in range(tries):\n",
    "        # we work with count many graphs\n",
    "        count = int(lenpo*perc)\n",
    "        # make copy of graphiterator\n",
    "        # select count random elements\n",
    "        # triplicate  the count long iterator\n",
    "        graphs_pos, graphs_pos_ = itertools.tee(graphs_pos)\n",
    "\n",
    "        # i pick only perc many :) \n",
    "        graphs_pos_,zzz = random_bipartition_iter(graphs_pos_, perc )    \n",
    "        graphs_pos_,graphs_pos__,graphs_pos___ = itertools.tee(graphs_pos_,3)\n",
    "\n",
    "        # do sampling\n",
    "        sampler.fit(graphs_pos__, n_jobs=-1, core_interface_pair_remove_threshold=3, interface_remove_threshold=3)\n",
    "\n",
    "        n_steps=20\n",
    "        sampling_factor=3 #how many samples per seed\n",
    "        sampling_interval=np.ceil([n_steps/sampling_factor])\n",
    "        improved_graphs = sampler.sample(graphs_pos_,\n",
    "                            same_radius=False,\n",
    "                            same_core_size=True,\n",
    "                            select_cip_max_tries=200,\n",
    "                            batch_size=30,\n",
    "                            n_steps=n_steps,\n",
    "                            sampling_interval=sampling_interval,\n",
    "                            n_jobs=-1,\n",
    "                            annealing_factor=1.0,\n",
    "                            probabilistic_core_choice=True)\n",
    "\n",
    "        #calculate the score of the improved versions\n",
    "        #calculate score of the originals\n",
    "        avg_imp=np.mean( [estimator.decision_function(e) for e in vectorizer.transform(unpack(improved_graphs)) ] )\n",
    "        avg_ori=np.mean( [estimator.decision_function(e) for e in vectorizer.transform(graphs_pos___)] )\n",
    "        samples.append(avg_imp)\n",
    "        originals.append(avg_ori)\n",
    "        \n",
    "    orires.append(originals)\n",
    "    samplres.append(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Original:\", orires\n",
    "print \"Sampled:\", samplres\n",
    "# ok alle infos sollten jetzt da sein um diesen supergraph zu zeichnen\n",
    "\n",
    "from graphlearn.utils.draw import plot_charts\n",
    "plot_charts(originals, data2=samples, xlabel=\"Training size\", ylabel=\"Score\", size=(10,4), log_scale=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
