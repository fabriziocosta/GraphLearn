{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from eden.util import configure_logging\n",
    "import logging\n",
    "configure_logging(logging.getLogger(),verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import tee, chain, islice\n",
    "import numpy as np\n",
    "import random\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from graphlearn.graphlearn import GraphLearnSampler\n",
    "from eden.util import fit,estimate\n",
    "from eden.graph import Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get data\n",
    "from eden.converter.graph.gspan import gspan_to_eden\n",
    "from itertools import islice\n",
    "def get_graphs(dataset_fname, size=100):\n",
    "    return  islice(gspan_to_eden(dataset_fname),size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_sample(graphs):\n",
    "    graphs, graphs_ = tee(graphs)\n",
    "    sampler=GraphLearnSampler(radius_list=[0,1],thickness_list=[1],\n",
    "                              min_cip_count=2, min_interface_count=2,\n",
    "                              vectorizer=Vectorizer(5))\n",
    "    \n",
    "    sampler.fit(graphs, nu=0.2, n_jobs=-1)\n",
    "\n",
    "    print('graph grammar stats:')\n",
    "    interface_counts, core_counts, cip_counts = sampler.grammar().size()\n",
    "    print('#interfaces: %d   #cores: %d   #core-interface-pairs: %d' % (interface_counts, core_counts, cip_counts))\n",
    "    graphs = sampler.sample(graphs_,\n",
    "                            same_radius=False,\n",
    "                            same_core_size=True,\n",
    "                            n_samples=3, n_steps=3,\n",
    "                            n_jobs=-1,\n",
    "                            accept_annealing_factor=1.5, accept_static_penalty=1,\n",
    "                            probabilistic_core_choice=True,\n",
    "                            generator_mode=True,\n",
    "                            select_cip_max_tries=400,\n",
    "                            keep_duplicates=True)\n",
    "    return graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fit_and_evaluate(pos_original, neg_original,\n",
    "                     pos_augmented, neg_augmented,\n",
    "                     pos_test, neg_test):\n",
    "    # create graph sets...orig augmented and sampled\n",
    "    pos_orig,pos_orig_ = tee(pos_original)\n",
    "    neg_orig,neg_orig_ = tee(neg_original)\n",
    "    \n",
    "    pos_augmented , pos_sampled = tee(pos_augmented)\n",
    "    neg_augmented , neg_sampled = tee(neg_augmented)\n",
    "    \n",
    "    pos_augmented = chain(pos_augmented,pos_orig_)\n",
    "    neg_augmented = chain(neg_augmented,neg_orig_)\n",
    "\n",
    "    predictive_performances = []\n",
    "    for desc,pos_train,neg_train in [('original',pos_orig, neg_orig),\n",
    "                                     ('sample',pos_sampled,neg_sampled),\n",
    "                                     ('original+sample',pos_augmented, neg_augmented)]:\n",
    "        pos_train,pos_train_ = tee(pos_train)\n",
    "        neg_train,neg_train_ = tee(neg_train)\n",
    "        pos_size=sum(1 for x in pos_train_)\n",
    "        neg_size=sum(1 for x in neg_train_)\n",
    "        if pos_size == 0 or neg_size == 0:\n",
    "            print \"-\"*80\n",
    "            print 'working on %s'%(desc)\n",
    "            print 'training set sizes: #pos: %d #neg: %d'%(pos_size, neg_size)\n",
    "            print 'WARNING: empty dataset'\n",
    "            predictive_performances.append(0)            \n",
    "        else:\n",
    "            start=time()\n",
    "            print \"-\"*80\n",
    "            print 'working on %s'%(desc)\n",
    "            print 'training set sizes: #pos: %d #neg: %d'%(pos_size, neg_size)\n",
    "            pos_test,pos_test_ = tee(pos_test)\n",
    "            neg_test,neg_test_ = tee(neg_test)\n",
    "            local_estimator = fit(pos_train, neg_train, Vectorizer(2), n_jobs=-1, n_iter_search=1)\n",
    "            apr, roc = estimate(pos_test_, neg_test_, local_estimator, Vectorizer(2))\n",
    "            predictive_performances.append(roc)\n",
    "            print 'elapsed: %.1f sec'%(time()-start)\n",
    "    return predictive_performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate(pos_fname, neg_fname, size=None, percentages=None, n_repetitions=None, train_test_split=None):\n",
    "    # initializing \n",
    "    graphs_pos = get_graphs(pos_fname, size=size)\n",
    "    graphs_neg = get_graphs(neg_fname, size=size)\n",
    "\n",
    "    # train/test split\n",
    "    from eden.util import random_bipartition_iter\n",
    "    pos_train_global,pos_test_global = random_bipartition_iter(graphs_pos,train_test_split)\n",
    "    neg_train_global,neg_test_global = random_bipartition_iter(graphs_neg,train_test_split)\n",
    "\n",
    "\n",
    "    original_repetitions = []\n",
    "    original_sample_repetitions = []\n",
    "    sample_repetitions = []\n",
    "\n",
    "    for percentage in percentages:\n",
    "        originals = []\n",
    "        originals_samples = []\n",
    "        samples = []\n",
    "        for repetition in range(n_repetitions):\n",
    "            pos_train_global,pos_train_global_ = tee(pos_train_global)\n",
    "            neg_train_global,neg_train_global_ = tee(neg_train_global)\n",
    "            pos_test_global,pos_test_global_ = tee(pos_test_global)\n",
    "            neg_test_global,neg_test_global_ = tee(neg_test_global)\n",
    "\n",
    "            # use shuffled list to create test and sample set\n",
    "            pos,pos_reminder = random_bipartition_iter(pos_train_global_,percentage)\n",
    "            pos,pos_ = tee(pos)\n",
    "            neg,neg_reminder = random_bipartition_iter(neg_train_global_,percentage)\n",
    "            neg,neg_ = tee(neg)\n",
    "\n",
    "            #sample independently from the 2 classes\n",
    "            print('Positive')\n",
    "            sampled_pos = fit_sample(pos)\n",
    "            print('Negative')\n",
    "            sampled_neg = fit_sample(neg)\n",
    "\n",
    "            #evaluate the predictive performance on held out test set\n",
    "            start=time()\n",
    "            print \"=\"*80\n",
    "            print 'repetition: %d/%d'%(repetition+1, n_repetitions)\n",
    "            print \"training percentage:\"+str(percentage)\n",
    "            perf_orig,\\\n",
    "            perf_samp,\\\n",
    "            perf_orig_samp = fit_and_evaluate(pos_,neg_,\n",
    "                                              sampled_pos,sampled_neg,\n",
    "                                              pos_test_global_,neg_test_global_)\n",
    "            print 'Time elapsed: %.1f sec'%((time()-start))\n",
    "            originals.append(perf_orig)\n",
    "            originals_samples.append(perf_orig_samp)\n",
    "            samples.append(perf_samp)\n",
    "\n",
    "        original_repetitions.append(originals)\n",
    "        original_sample_repetitions.append(originals_samples)\n",
    "        sample_repetitions.append(samples)\n",
    "    \n",
    "    return original_repetitions, original_sample_repetitions, sample_repetitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with dataset: IGROV1_t\n",
      "Positive\n",
      "graph grammar stats:\n",
      "#interfaces: 51   #cores: 54   #core-interface-pairs: 219\n",
      "Negative\n",
      "graph grammar stats:\n",
      "#interfaces: 41   #cores: 52   #core-interface-pairs: 159\n",
      "================================================================================\n",
      "repetition: 1/5\n",
      "training percentage:0.2\n",
      "--------------------------------------------------------------------------------\n",
      "working on original\n",
      "training set sizes: #pos: 56 #neg: 56\n",
      "Test set\n",
      "Instances: 240 ; Features: 1048577 with an avg of 109 features per instance\n",
      "--------------------------------------------------------------------------------\n",
      "Test Estimate\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.70      0.68      0.69       120\n",
      "          1       0.69      0.71      0.70       120\n",
      "\n",
      "avg / total       0.70      0.70      0.70       240\n",
      "\n",
      "APR: 0.665\n",
      "ROC: 0.731\n",
      "elapsed: 2.5 sec\n",
      "--------------------------------------------------------------------------------\n",
      "working on sample\n",
      "training set sizes: #pos: 112 #neg: 108\n",
      "Test set\n",
      "Instances: 240 ; Features: 1048577 with an avg of 109 features per instance\n",
      "--------------------------------------------------------------------------------\n",
      "Test Estimate\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.61      0.53      0.56       120\n",
      "          1       0.58      0.66      0.62       120\n",
      "\n",
      "avg / total       0.59      0.59      0.59       240\n",
      "\n",
      "APR: 0.614\n",
      "ROC: 0.663\n",
      "elapsed: 2.6 sec\n",
      "--------------------------------------------------------------------------------\n",
      "working on original+sample\n",
      "training set sizes: #pos: 168 #neg: 164\n",
      "Test set\n",
      "Instances: 240 ; Features: 1048577 with an avg of 109 features per instance\n",
      "--------------------------------------------------------------------------------\n",
      "Test Estimate\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.68      0.59      0.63       120\n",
      "          1       0.64      0.72      0.67       120\n",
      "\n",
      "avg / total       0.66      0.65      0.65       240\n",
      "\n",
      "APR: 0.640\n",
      "ROC: 0.697\n",
      "elapsed: 3.0 sec\n",
      "Time elapsed: 14.1 sec\n",
      "Positive\n",
      "graph grammar stats:\n",
      "#interfaces: 51   #cores: 54   #core-interface-pairs: 219\n",
      "Negative\n",
      "graph grammar stats:\n",
      "#interfaces: 41   #cores: 52   #core-interface-pairs: 159\n",
      "================================================================================\n",
      "repetition: 2/5\n",
      "training percentage:0.2\n",
      "--------------------------------------------------------------------------------\n",
      "working on original\n",
      "training set sizes: #pos: 56 #neg: 56\n",
      "Test set\n",
      "Instances: 240 ; Features: 1048577 with an avg of 109 features per instance\n",
      "--------------------------------------------------------------------------------\n",
      "Test Estimate\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.71      0.58      0.64       120\n",
      "          1       0.65      0.76      0.70       120\n",
      "\n",
      "avg / total       0.68      0.67      0.67       240\n",
      "\n",
      "APR: 0.676\n",
      "ROC: 0.743\n",
      "elapsed: 2.4 sec\n",
      "--------------------------------------------------------------------------------\n",
      "working on sample\n",
      "training set sizes: #pos: 112 #neg: 111\n",
      "Test set\n",
      "Instances: 240 ; Features: 1048577 with an avg of 109 features per instance\n",
      "--------------------------------------------------------------------------------\n",
      "Test Estimate\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.67      0.59      0.63       120\n",
      "          1       0.63      0.71      0.67       120\n",
      "\n",
      "avg / total       0.65      0.65      0.65       240\n",
      "\n",
      "APR: 0.645\n",
      "ROC: 0.702\n",
      "elapsed: 2.7 sec\n",
      "--------------------------------------------------------------------------------\n",
      "working on original+sample\n",
      "training set sizes: #pos: 168 #neg: 167\n",
      "Test set\n",
      "Instances: 240 ; Features: 1048577 with an avg of 109 features per instance\n",
      "--------------------------------------------------------------------------------\n",
      "Test Estimate\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.67      0.65      0.66       120\n",
      "          1       0.66      0.68      0.67       120\n",
      "\n",
      "avg / total       0.66      0.66      0.66       240\n",
      "\n",
      "APR: 0.651\n",
      "ROC: 0.712\n",
      "elapsed: 3.1 sec\n",
      "Time elapsed: 14.3 sec\n",
      "Positive\n",
      "graph grammar stats:\n",
      "#interfaces: 51   #cores: 54   #core-interface-pairs: 219\n",
      "Negative\n",
      "graph grammar stats:\n",
      "#interfaces: 41   #cores: 52   #core-interface-pairs: 159\n",
      "================================================================================\n",
      "repetition: 3/5\n",
      "training percentage:0.2\n",
      "--------------------------------------------------------------------------------\n",
      "working on original\n",
      "training set sizes: #pos: 56 #neg: 56\n",
      "Test set\n",
      "Instances: 240 ; Features: 1048577 with an avg of 109 features per instance\n",
      "--------------------------------------------------------------------------------\n",
      "Test Estimate\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.67      0.64      0.66       120\n",
      "          1       0.66      0.68      0.67       120\n",
      "\n",
      "avg / total       0.66      0.66      0.66       240\n",
      "\n",
      "APR: 0.677\n",
      "ROC: 0.728\n",
      "elapsed: 2.4 sec\n",
      "--------------------------------------------------------------------------------\n",
      "working on sample\n",
      "training set sizes: #pos: 112 #neg: 110\n",
      "Test set\n",
      "Instances: 240 ; Features: 1048577 with an avg of 109 features per instance\n",
      "--------------------------------------------------------------------------------\n",
      "Test Estimate\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.62      0.57      0.59       120\n",
      "          1       0.60      0.66      0.63       120\n",
      "\n",
      "avg / total       0.61      0.61      0.61       240\n",
      "\n",
      "APR: 0.596\n",
      "ROC: 0.642\n",
      "elapsed: 2.7 sec\n",
      "--------------------------------------------------------------------------------\n",
      "working on original+sample\n",
      "training set sizes: #pos: 168 #neg: 166\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#experiment\n",
    "\n",
    "dataset_names = !cat NCI60/names\n",
    "dataset = dataset_names[6]\n",
    "print 'Working with dataset: %s' % dataset \n",
    "pos_dataset_fname = 'NCI60/' + dataset + '_orig_pos.gspan'\n",
    "neg_dataset_fname = 'NCI60/' + dataset + '_orig_neg.gspan'\n",
    "\n",
    "#pos_dataset_fname = 'bursi.pos.gspan'\n",
    "#neg_dataset_fname = 'bursi.neg.gspan'\n",
    "\n",
    "\n",
    "percentages=[.2,.4,.6,.8]\n",
    "original_repetitions,\\\n",
    "original_sample_repetitions,\\\n",
    "sample_repetitions = evaluate(pos_dataset_fname,neg_dataset_fname,\n",
    "                              size=400,\n",
    "                              percentages=percentages,\n",
    "                              n_repetitions=5,\n",
    "                              train_test_split=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot\n",
    "os = np.mean(original_sample_repetitions, axis=1)\n",
    "o = np.mean(original_repetitions, axis=1)\n",
    "s = np.mean(sample_repetitions, axis=1)\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.grid()\n",
    "plt.plot(os, color='g', marker='o', markersize=6, markeredgecolor='g', markerfacecolor='w', label='original+sample')\n",
    "plt.plot(o, color='r', marker='o', markersize=6, markeredgecolor='r', markerfacecolor='w', label='original')\n",
    "plt.plot(s, color='b', marker='o', markersize=6, markeredgecolor='b', markerfacecolor='w', label='sample')\n",
    "plt.legend(loc='lower right')\n",
    "plt.savefig('%s_plot_predictive_performance_of_samples.pdf' % dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot\n",
    "from graphlearn.utils.draw import draw_learning_curve\n",
    "draw_learning_curve(data_first=original_repetitions,\n",
    "                    data_second=original_sample_repetitions,\n",
    "                    x_axis=percentages,\n",
    "                    measure='roc',\n",
    "                    delta=0.005,scaling=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
